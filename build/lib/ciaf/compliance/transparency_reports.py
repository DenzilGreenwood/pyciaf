"""
Transparency Reports Module for CIAF

This module provides automated generation of transparency reports
for AI models, including algorithmic transparency, decision explanations,
and public disclosure documents.

Created: 2025-09-09
Last Modified: 2025-09-11
Author: Denzil James Greenwood
Version: 1.0.0
"""

import json
import os
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

from .audit_trails import AuditEventType, AuditTrailGenerator, ComplianceAuditRecord
from .regulatory_mapping import ComplianceFramework, RegulatoryMapper
from .risk_assessment import ComprehensiveRiskAssessment, RiskAssessmentEngine


class TransparencyLevel(Enum):
    """Levels of transparency disclosure."""

    PUBLIC = "public"
    REGULATORY = "regulatory"
    INTERNAL = "internal"
    TECHNICAL = "technical"


class ReportAudience(Enum):
    """Target audience for transparency reports."""

    GENERAL_PUBLIC = "general_public"
    REGULATORS = "regulators"
    TECHNICAL_AUDITORS = "technical_auditors"
    AFFECTED_INDIVIDUALS = "affected_individuals"
    CIVIL_SOCIETY = "civil_society"
    RESEARCHERS = "researchers"


@dataclass
class AlgorithmicTransparencyMetrics:
    """Metrics for algorithmic transparency."""

    model_name: str
    model_version: str
    model_type: str
    training_data_summary: Dict[str, Any]
    performance_metrics: Dict[str, float]
    bias_metrics: Dict[str, float]
    fairness_indicators: Dict[str, float]
    explainability_coverage: float  # Percentage of decisions with explanations
    decision_boundary_analysis: Dict[str, Any]
    feature_importance: Dict[str, float]
    uncertainty_quantification: Dict[str, float]
    last_updated: str


@dataclass
class DecisionExplanation:
    """Individual decision explanation."""

    decision_id: str
    timestamp: str
    input_data_summary: Dict[str, Any]
    prediction: Any
    confidence_score: float
    explanation_method: str
    feature_contributions: Dict[str, float]
    decision_rationale: str
    alternative_outcomes: List[Dict[str, Any]]
    uncertainty_factors: List[str]
    bias_check_results: Dict[str, Any]
    human_review_required: bool


@dataclass
class TransparencyReport:
    """Comprehensive transparency report."""

    report_id: str
    title: str
    model_name: str
    model_version: str
    reporting_period: Dict[str, str]
    transparency_level: TransparencyLevel
    target_audience: ReportAudience
    report_sections: List[Dict[str, Any]]
    algorithmic_metrics: AlgorithmicTransparencyMetrics
    decision_explanations_sample: List[DecisionExplanation]
    public_interest_assessments: List[Dict[str, Any]]
    accountability_measures: Dict[str, Any]
    contact_information: Dict[str, str]
    publication_date: str
    next_report_due: str

    def to_html(self) -> str:
        """Convert transparency report to HTML."""
        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>{self.title}</title>
    <meta charset="UTF-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; color: #333; }}
        .header {{ background: #f8f9fa; padding: 30px; border-radius: 8px; margin-bottom: 30px; }}
        .section {{ margin-bottom: 40px; padding: 20px; border-left: 4px solid #007acc; background: #f9f9f9; }}
        .metrics-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; }}
        .metric-card {{ background: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .transparency-badge {{ display: inline-block; padding: 5px 10px; background: #28a745; color: white; border-radius: 3px; font-size: 0.8em; }}
        .explanation {{ background: #e9ecef; padding: 15px; border-radius: 5px; margin: 10px 0; }}
        h1 {{ color: #2c3e50; }}
        h2 {{ color: #34495e; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }}
        h3 {{ color: #5d6d7e; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f8f9fa; }}
        .contact-info {{ background: #fff3cd; padding: 20px; border-radius: 5px; margin-top: 30px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>{self.title}</h1>
        <p><strong>Model:</strong> {self.model_name} (Version: {self.model_version})</p>
        <p><strong>Reporting Period:</strong> {self.reporting_period.get('start', 'N/A')} to {self.reporting_period.get('end', 'N/A')}</p>
        <p><strong>Transparency Level:</strong> <span class="transparency-badge">{self.transparency_level.value.title()}</span></p>
        <p><strong>Target Audience:</strong> {self.target_audience.value.replace('_', ' ').title()}</p>
        <p><strong>Published:</strong> {self.publication_date[:10]}</p>
    </div>
"""

        # Executive Summary
        html += self._generate_executive_summary()

        # Algorithmic Transparency Section
        html += self._generate_algorithmic_transparency_section()

        # Performance and Fairness Metrics
        html += self._generate_performance_section()

        # Decision Explanations
        html += self._generate_explanations_section()

        # Public Interest Assessment
        html += self._generate_public_interest_section()

        # Accountability Measures
        html += self._generate_accountability_section()

        # Contact Information
        html += self._generate_contact_section()

        html += """
</body>
</html>"""
        return html

    def _generate_executive_summary(self) -> str:
        """Generate executive summary section."""
        return f"""
    <div class="section">
        <h2>Executive Summary</h2>
        <p>This transparency report provides comprehensive information about the {self.model_name} 
        AI model's operation, performance, and impact during the reporting period. The report aims to 
        provide stakeholders with clear insights into how the model makes decisions, its performance 
        characteristics, and measures taken to ensure fairness and accountability.</p>
        
        <h3>Key Highlights</h3>
        <ul>
            <li><strong>Explainability Coverage:</strong> {self.algorithmic_metrics.explainability_coverage:.1%} of decisions include explanations</li>
            <li><strong>Performance:</strong> Overall accuracy of {self.algorithmic_metrics.performance_metrics.get('accuracy', 0):.1%}</li>
            <li><strong>Fairness:</strong> Bias metrics within acceptable thresholds</li>
            <li><strong>Transparency Level:</strong> {self.transparency_level.value.title()} disclosure</li>
        </ul>
    </div>"""

    def _generate_algorithmic_transparency_section(self) -> str:
        """Generate algorithmic transparency section."""
        return f"""
    <div class="section">
        <h2>Algorithmic Transparency</h2>
        
        <h3>Model Overview</h3>
        <table>
            <tr><th>Property</th><th>Value</th></tr>
            <tr><td>Model Type</td><td>{self.algorithmic_metrics.model_type}</td></tr>
            <tr><td>Version</td><td>{self.algorithmic_metrics.model_version}</td></tr>
            <tr><td>Last Updated</td><td>{self.algorithmic_metrics.last_updated[:10]}</td></tr>
            <tr><td>Training Data Size</td><td>{self.algorithmic_metrics.training_data_summary.get('size', 'N/A')}</td></tr>
        </table>
        
        <h3>Feature Importance</h3>
        <p>The following features have the highest impact on model decisions:</p>
        <ol>
        {"".join([f"<li><strong>{feature}:</strong> {importance:.3f}</li>" 
                 for feature, importance in sorted(self.algorithmic_metrics.feature_importance.items(), 
                                                 key=lambda x: x[1], reverse=True)[:10]])}
        </ol>
        
        <h3>Decision Boundaries</h3>
        <p>The model's decision boundaries have been analyzed to ensure consistency and fairness. 
        Key findings include:</p>
        <ul>
        {"".join([f"<li>{key}: {value}</li>" 
                 for key, value in self.algorithmic_metrics.decision_boundary_analysis.items()])}
        </ul>
    </div>"""

    def _generate_performance_section(self) -> str:
        """Generate performance and fairness section."""
        return f"""
    <div class="section">
        <h2>Performance and Fairness Metrics</h2>
        
        <div class="metrics-grid">
            <div class="metric-card">
                <h4>Performance Metrics</h4>
                {"".join([f"<p><strong>{metric.title()}:</strong> {value:.3f}</p>" 
                         for metric, value in self.algorithmic_metrics.performance_metrics.items()])}
            </div>
            
            <div class="metric-card">
                <h4>Bias Metrics</h4>
                {"".join([f"<p><strong>{metric.title()}:</strong> {value:.3f}</p>" 
                         for metric, value in self.algorithmic_metrics.bias_metrics.items()])}
            </div>
            
            <div class="metric-card">
                <h4>Fairness Indicators</h4>
                {"".join([f"<p><strong>{metric.title()}:</strong> {value:.3f}</p>" 
                         for metric, value in self.algorithmic_metrics.fairness_indicators.items()])}
            </div>
            
            <div class="metric-card">
                <h4>Uncertainty Quantification</h4>
                {"".join([f"<p><strong>{metric.title()}:</strong> {value:.3f}</p>" 
                         for metric, value in self.algorithmic_metrics.uncertainty_quantification.items()])}
            </div>
        </div>
    </div>"""

    def _generate_explanations_section(self) -> str:
        """Generate decision explanations section."""
        explanations_html = (
            """
    <div class="section">
        <h2>Decision Explanations</h2>
        <p>The following section provides sample explanations for model decisions to illustrate 
        how the AI system makes its determinations.</p>
        
        <h3>Explanation Coverage</h3>
        <p>Explanations are provided for """
            + f"{self.algorithmic_metrics.explainability_coverage:.1%}"
            + """ of all decisions.</p>
        
        <h3>Sample Explanations</h3>"""
        )

        for i, explanation in enumerate(self.decision_explanations_sample[:3], 1):
            explanations_html += f"""
        <div class="explanation">
            <h4>Decision Example {i}</h4>
            <p><strong>Decision ID:</strong> {explanation.decision_id}</p>
            <p><strong>Timestamp:</strong> {explanation.timestamp[:19]}</p>
            <p><strong>Prediction:</strong> {explanation.prediction}</p>
            <p><strong>Confidence:</strong> {explanation.confidence_score:.2%}</p>
            <p><strong>Method:</strong> {explanation.explanation_method}</p>
            <p><strong>Rationale:</strong> {explanation.decision_rationale}</p>
            
            <h5>Top Contributing Features:</h5>
            <ul>
            {"".join([f"<li><strong>{feature}:</strong> {contribution:.3f}</li>" 
                     for feature, contribution in sorted(explanation.feature_contributions.items(), 
                                                       key=lambda x: abs(x[1]), reverse=True)[:5]])}
            </ul>
            
            {f"<p><strong>Human Review Required:</strong> {'Yes' if explanation.human_review_required else 'No'}</p>" if explanation.human_review_required else ""}
        </div>"""

        explanations_html += "</div>"
        return explanations_html

    def _generate_public_interest_section(self) -> str:
        """Generate public interest assessment section."""
        return f"""
    <div class="section">
        <h2>Public Interest Assessment</h2>
        <p>This section addresses the potential impacts of the AI model on public interest and society.</p>
        
        {"".join([f'''
        <h3>{assessment.get('title', 'Assessment')}</h3>
        <p>{assessment.get('description', 'No description available')}</p>
        <p><strong>Impact Level:</strong> {assessment.get('impact_level', 'Unknown')}</p>
        <p><strong>Mitigation Measures:</strong> {', '.join(assessment.get('mitigation_measures', []))}</p>
        ''' for assessment in self.public_interest_assessments])}
    </div>"""

    def _generate_accountability_section(self) -> str:
        """Generate accountability measures section."""
        return f"""
    <div class="section">
        <h2>Accountability Measures</h2>
        <p>The following measures are in place to ensure responsible AI deployment and operation:</p>
        
        <h3>Governance Structure</h3>
        <p><strong>Responsible Team:</strong> {self.accountability_measures.get('responsible_team', 'Not specified')}</p>
        <p><strong>Oversight Body:</strong> {self.accountability_measures.get('oversight_body', 'Not specified')}</p>
        
        <h3>Monitoring and Review</h3>
        <ul>
        {"".join([f"<li>{measure}</li>" for measure in self.accountability_measures.get('monitoring_measures', [])])}
        </ul>
        
        <h3>Redress Mechanisms</h3>
        <ul>
        {"".join([f"<li>{mechanism}</li>" for mechanism in self.accountability_measures.get('redress_mechanisms', [])])}
        </ul>
        
        <h3>Regular Reviews</h3>
        <p><strong>Review Frequency:</strong> {self.accountability_measures.get('review_frequency', 'Not specified')}</p>
        <p><strong>Next Review Due:</strong> {self.next_report_due[:10]}</p>
    </div>"""

    def _generate_contact_section(self) -> str:
        """Generate contact information section."""
        return f"""
    <div class="contact-info">
        <h2>Contact Information</h2>
        <p>For questions, concerns, or feedback about this AI model and its transparency report:</p>
        
        <p><strong>Primary Contact:</strong> {self.contact_information.get('primary_contact', 'Not provided')}</p>
        <p><strong>Email:</strong> {self.contact_information.get('email', 'Not provided')}</p>
        <p><strong>Phone:</strong> {self.contact_information.get('phone', 'Not provided')}</p>
        <p><strong>Address:</strong> {self.contact_information.get('address', 'Not provided')}</p>
        
        <p><strong>Data Protection Officer:</strong> {self.contact_information.get('dpo', 'Not specified')}</p>
        <p><strong>Ethics Committee:</strong> {self.contact_information.get('ethics_committee', 'Not specified')}</p>
        
        <p><em>This report was generated automatically by the CIAF Transparency Reporting System.</em></p>
    </div>"""


class TransparencyReportGenerator:
    """Automated transparency report generator."""

    def __init__(self, model_name: str):
        """Initialize transparency report generator."""
        self.model_name = model_name
        self.regulatory_mapper = RegulatoryMapper()
        self.generated_reports: List[TransparencyReport] = []

    def generate_public_transparency_report(
        self,
        model_version: str,
        audit_generator: AuditTrailGenerator,
        risk_engine: RiskAssessmentEngine,
        reporting_period_days: int = 90,
    ) -> TransparencyReport:
        """Generate public transparency report."""

        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=reporting_period_days)

        # Collect audit data
        audit_events = audit_generator.get_audit_trail(start_date, end_date)
        prediction_events = [
            e for e in audit_events if e.event_type == AuditEventType.MODEL_PREDICTION
        ]

        # Generate algorithmic transparency metrics
        algorithmic_metrics = self._generate_algorithmic_metrics(
            model_version, audit_events, prediction_events
        )

        # Generate sample decision explanations
        decision_explanations = self._generate_decision_explanations(
            prediction_events[:5]
        )

        # Generate public interest assessments
        public_interest_assessments = self._generate_public_interest_assessments(
            audit_events
        )

        # Define accountability measures
        accountability_measures = {
            "responsible_team": "AI Ethics and Governance Team",
            "oversight_body": "AI Oversight Committee",
            "monitoring_measures": [
                "Continuous performance monitoring",
                "Weekly bias assessment",
                "Monthly fairness audits",
                "Quarterly comprehensive reviews",
            ],
            "redress_mechanisms": [
                "Individual decision appeals process",
                "Public feedback portal",
                "Ethics committee review",
                "External audit process",
            ],
            "review_frequency": "Quarterly",
        }

        # Contact information
        contact_info = {
            "primary_contact": "AI Transparency Team",
            "email": "ai-transparency@organization.com",
            "phone": "+1-555-0199",
            "address": "123 AI Ethics Street, Tech City, TC 12345",
            "dpo": "Chief Data Protection Officer",
            "ethics_committee": "AI Ethics Committee",
        }

        report = TransparencyReport(
            report_id=f"TRANSPARENCY_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            title=f"AI Transparency Report - {self.model_name}",
            model_name=self.model_name,
            model_version=model_version,
            reporting_period={
                "start": start_date.isoformat()[:10],
                "end": end_date.isoformat()[:10],
                "days": reporting_period_days,
            },
            transparency_level=TransparencyLevel.PUBLIC,
            target_audience=ReportAudience.GENERAL_PUBLIC,
            report_sections=[],  # Sections are generated in HTML/markdown
            algorithmic_metrics=algorithmic_metrics,
            decision_explanations_sample=decision_explanations,
            public_interest_assessments=public_interest_assessments,
            accountability_measures=accountability_measures,
            contact_information=contact_info,
            publication_date=datetime.now(timezone.utc).isoformat(),
            next_report_due=(end_date + timedelta(days=90)).isoformat(),
        )

        self.generated_reports.append(report)
        return report

    def generate_regulatory_transparency_report(
        self,
        framework: ComplianceFramework,
        model_version: str,
        audit_generator: AuditTrailGenerator,
        risk_engine: RiskAssessmentEngine,
        reporting_period_days: int = 90,
    ) -> TransparencyReport:
        """Generate regulatory transparency report."""

        # Similar to public report but with more technical detail
        public_report = self.generate_public_transparency_report(
            model_version, audit_generator, risk_engine, reporting_period_days
        )

        # Modify for regulatory audience
        regulatory_report = TransparencyReport(
            report_id=f"REG_TRANSPARENCY_{framework.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            title=f"Regulatory Transparency Report - {framework.value} - {self.model_name}",
            model_name=public_report.model_name,
            model_version=public_report.model_version,
            reporting_period=public_report.reporting_period,
            transparency_level=TransparencyLevel.REGULATORY,
            target_audience=ReportAudience.REGULATORS,
            report_sections=public_report.report_sections,
            algorithmic_metrics=public_report.algorithmic_metrics,
            decision_explanations_sample=public_report.decision_explanations_sample,
            public_interest_assessments=public_report.public_interest_assessments,
            accountability_measures=public_report.accountability_measures,
            contact_information=public_report.contact_information,
            publication_date=datetime.now(timezone.utc).isoformat(),
            next_report_due=public_report.next_report_due,
        )

        # Add regulatory-specific contact
        regulatory_report.contact_information["regulatory_liaison"] = (
            "Chief Compliance Officer"
        )
        regulatory_report.contact_information["regulatory_email"] = (
            "compliance@organization.com"
        )

        self.generated_reports.append(regulatory_report)
        return regulatory_report

    def generate_technical_transparency_report(
        self,
        model_version: str,
        audit_generator: AuditTrailGenerator,
        risk_engine: RiskAssessmentEngine,
        reporting_period_days: int = 30,
    ) -> TransparencyReport:
        """Generate technical transparency report for technical auditors."""

        base_report = self.generate_public_transparency_report(
            model_version, audit_generator, risk_engine, reporting_period_days
        )

        # Enhanced algorithmic metrics for technical audience
        enhanced_metrics = base_report.algorithmic_metrics
        enhanced_metrics.decision_boundary_analysis.update(
            {
                "model_architecture": "Deep Neural Network with attention mechanism",
                "training_methodology": "Supervised learning with cross-validation",
                "hyperparameters": {
                    "learning_rate": 0.001,
                    "batch_size": 32,
                    "epochs": 100,
                },
                "validation_methodology": "5-fold cross-validation",
                "test_set_performance": {
                    "accuracy": 0.89,
                    "precision": 0.87,
                    "recall": 0.91,
                },
            }
        )

        technical_report = TransparencyReport(
            report_id=f"TECH_TRANSPARENCY_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            title=f"Technical Transparency Report - {self.model_name}",
            model_name=base_report.model_name,
            model_version=base_report.model_version,
            reporting_period=base_report.reporting_period,
            transparency_level=TransparencyLevel.TECHNICAL,
            target_audience=ReportAudience.TECHNICAL_AUDITORS,
            report_sections=base_report.report_sections,
            algorithmic_metrics=enhanced_metrics,
            decision_explanations_sample=base_report.decision_explanations_sample,
            public_interest_assessments=base_report.public_interest_assessments,
            accountability_measures=base_report.accountability_measures,
            contact_information=base_report.contact_information,
            publication_date=datetime.now(timezone.utc).isoformat(),
            next_report_due=base_report.next_report_due,
        )

        # Add technical contact
        technical_report.contact_information["technical_lead"] = "Chief AI Officer"
        technical_report.contact_information["technical_email"] = (
            "ai-tech@organization.com"
        )

        self.generated_reports.append(technical_report)
        return technical_report

    def _generate_algorithmic_metrics(
        self,
        model_version: str,
        audit_events: List[ComplianceAuditRecord],
        prediction_events: List[ComplianceAuditRecord],
    ) -> AlgorithmicTransparencyMetrics:
        """Generate algorithmic transparency metrics."""

        # Calculate explainability coverage
        explained_predictions = [
            e for e in prediction_events if e.metadata.get("explanation")
        ]
        explainability_coverage = (
            len(explained_predictions) / len(prediction_events)
            if prediction_events
            else 0
        )

        # Simulate metrics (in practice, these would be calculated from real data)
        performance_metrics = {
            "accuracy": 0.87,
            "precision": 0.84,
            "recall": 0.89,
            "f1_score": 0.86,
            "auc_roc": 0.91,
        }

        bias_metrics = {
            "demographic_parity": 0.08,
            "equalized_odds": 0.06,
            "statistical_parity": 0.09,
            "individual_fairness": 0.92,
        }

        fairness_indicators = {
            "overall_fairness_score": 0.88,
            "disparate_impact_ratio": 0.89,
            "calibration_score": 0.91,
            "treatment_equality": 0.87,
        }

        feature_importance = {
            "feature_1": 0.23,
            "feature_2": 0.18,
            "feature_3": 0.15,
            "feature_4": 0.12,
            "feature_5": 0.10,
            "feature_6": 0.08,
            "feature_7": 0.07,
            "feature_8": 0.04,
            "feature_9": 0.02,
            "feature_10": 0.01,
        }

        uncertainty_quantification = {
            "prediction_uncertainty": 0.15,
            "epistemic_uncertainty": 0.08,
            "aleatoric_uncertainty": 0.12,
            "confidence_calibration": 0.89,
        }

        decision_boundary_analysis = {
            "boundary_complexity": "medium",
            "decision_regions": 5,
            "overlap_regions": 2,
            "stability_score": 0.91,
        }

        return AlgorithmicTransparencyMetrics(
            model_name=self.model_name,
            model_version=model_version,
            model_type="Supervised Learning - Neural Network",
            training_data_summary={
                "size": "100,000 samples",
                "features": 50,
                "data_sources": ["Internal database", "Public datasets"],
                "collection_period": "2023-01-01 to 2023-12-31",
            },
            performance_metrics=performance_metrics,
            bias_metrics=bias_metrics,
            fairness_indicators=fairness_indicators,
            explainability_coverage=explainability_coverage,
            decision_boundary_analysis=decision_boundary_analysis,
            feature_importance=feature_importance,
            uncertainty_quantification=uncertainty_quantification,
            last_updated=datetime.now(timezone.utc).isoformat(),
        )

    def _generate_decision_explanations(
        self, prediction_events: List[ComplianceAuditRecord]
    ) -> List[DecisionExplanation]:
        """Generate sample decision explanations."""

        explanations = []

        for i, event in enumerate(prediction_events):
            explanation = DecisionExplanation(
                decision_id=event.event_id,
                timestamp=event.timestamp,
                input_data_summary={
                    "features_count": 10,
                    "data_types": ["numerical", "categorical", "text"],
                    "missing_values": 0,
                },
                prediction=f"Class_{i % 3}",  # Simulate classification
                confidence_score=0.85 + (i * 0.03) % 0.15,  # Simulate confidence
                explanation_method="SHAP (SHapley Additive exPlanations)",
                feature_contributions={
                    f"feature_{j+1}": (0.1 + (j * 0.05)) * (1 if j % 2 == 0 else -1)
                    for j in range(5)
                },
                decision_rationale=f"The model predicted Class_{i % 3} based on the combination of feature values, with feature_1 being the most influential positive contributor.",
                alternative_outcomes=[
                    {
                        "outcome": f"Class_{(i+1) % 3}",
                        "probability": 0.15 - (i * 0.02) % 0.10,
                    },
                    {
                        "outcome": f"Class_{(i+2) % 3}",
                        "probability": 0.05 + (i * 0.01) % 0.05,
                    },
                ],
                uncertainty_factors=[
                    "Limited training data for this region",
                    "Feature correlation effects",
                ],
                bias_check_results={
                    "bias_detected": False,
                    "protected_attributes_checked": ["age", "gender"],
                    "fairness_score": 0.91,
                },
                human_review_required=i % 4 == 0,  # Every 4th decision requires review
            )
            explanations.append(explanation)

        return explanations

    def _generate_public_interest_assessments(
        self, audit_events: List[ComplianceAuditRecord]
    ) -> List[Dict[str, Any]]:
        """Generate public interest assessments."""

        return [
            {
                "title": "Economic Impact Assessment",
                "description": "The AI model's deployment has potential economic implications for various stakeholders. Analysis shows neutral to positive economic impact.",
                "impact_level": "Medium",
                "mitigation_measures": [
                    "Regular economic impact monitoring",
                    "Stakeholder engagement",
                    "Impact assessment reviews",
                ],
                "affected_groups": ["Job seekers", "Employers", "Service providers"],
            },
            {
                "title": "Social Equity Assessment",
                "description": "Evaluation of the model's impact on social equity and fair treatment across different demographic groups.",
                "impact_level": "Medium",
                "mitigation_measures": [
                    "Bias monitoring",
                    "Fairness improvements",
                    "Community feedback integration",
                ],
                "affected_groups": [
                    "Minority communities",
                    "Low-income populations",
                    "Elderly citizens",
                ],
            },
            {
                "title": "Privacy and Civil Liberties Assessment",
                "description": "Assessment of potential impacts on individual privacy rights and civil liberties.",
                "impact_level": "Low",
                "mitigation_measures": [
                    "Privacy-preserving techniques",
                    "Data minimization",
                    "Consent management",
                ],
                "affected_groups": ["All data subjects", "Privacy advocates"],
            },
        ]

    def save_transparency_report(
        self, report: TransparencyReport, output_dir: str, format: str = "html"
    ) -> str:
        """Save transparency report to file."""

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        filename = f"{report.report_id}.{format.lower()}"
        filepath = os.path.join(output_dir, filename)

        if format.lower() == "html":
            content = report.to_html()
        elif format.lower() == "json":
            content = self._report_to_json(report)
        else:
            raise ValueError(f"Unsupported format: {format}")

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

        return filepath

    def _report_to_json(self, report: TransparencyReport) -> str:
        """Convert transparency report to JSON."""
        report_dict = {
            "report_id": report.report_id,
            "title": report.title,
            "model_name": report.model_name,
            "model_version": report.model_version,
            "reporting_period": report.reporting_period,
            "transparency_level": report.transparency_level.value,
            "target_audience": report.target_audience.value,
            "algorithmic_metrics": {
                "model_type": report.algorithmic_metrics.model_type,
                "explainability_coverage": report.algorithmic_metrics.explainability_coverage,
                "performance_metrics": report.algorithmic_metrics.performance_metrics,
                "bias_metrics": report.algorithmic_metrics.bias_metrics,
                "fairness_indicators": report.algorithmic_metrics.fairness_indicators,
                "feature_importance": report.algorithmic_metrics.feature_importance,
                "uncertainty_quantification": report.algorithmic_metrics.uncertainty_quantification,
            },
            "decision_explanations_count": len(report.decision_explanations_sample),
            "public_interest_assessments": report.public_interest_assessments,
            "accountability_measures": report.accountability_measures,
            "contact_information": report.contact_information,
            "publication_date": report.publication_date,
            "next_report_due": report.next_report_due,
        }

        return json.dumps(report_dict, indent=2)

    def generate_transparency_dashboard_data(self) -> Dict[str, Any]:
        """Generate data for transparency dashboard."""

        if not self.generated_reports:
            return {"message": "No transparency reports available"}

        latest_report = self.generated_reports[-1]

        dashboard_data = {
            "model_overview": {
                "name": latest_report.model_name,
                "version": latest_report.model_version,
                "type": latest_report.algorithmic_metrics.model_type,
                "last_updated": latest_report.algorithmic_metrics.last_updated[:10],
            },
            "transparency_metrics": {
                "explainability_coverage": latest_report.algorithmic_metrics.explainability_coverage,
                "overall_performance": latest_report.algorithmic_metrics.performance_metrics.get(
                    "accuracy", 0
                ),
                "fairness_score": latest_report.algorithmic_metrics.fairness_indicators.get(
                    "overall_fairness_score", 0
                ),
                "bias_score": (
                    max(latest_report.algorithmic_metrics.bias_metrics.values())
                    if latest_report.algorithmic_metrics.bias_metrics
                    else 0
                ),
            },
            "reporting_status": {
                "reports_generated": len(self.generated_reports),
                "last_report_date": latest_report.publication_date[:10],
                "next_report_due": latest_report.next_report_due[:10],
                "compliance_status": "compliant",
            },
            "public_interest": {
                "assessments_conducted": len(latest_report.public_interest_assessments),
                "impact_level": "medium",
                "mitigation_measures_active": sum(
                    len(assessment.get("mitigation_measures", []))
                    for assessment in latest_report.public_interest_assessments
                ),
            },
        }

        return dashboard_data

    def get_generated_reports(self) -> List[TransparencyReport]:
        """Get list of all generated transparency reports."""
        return self.generated_reports

    def clear_reports(self):
        """Clear all generated reports."""
        self.generated_reports.clear()
